{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901009c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff00c158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "import argparse\n",
    "from zipfile import ZipFile\n",
    "from urllib.request import urlopen\n",
    "import shutil\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from datetime import datetime\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau, TensorBoard, CSVLogger\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import csv\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from keras import backend as K\n",
    "from skimage.io import imread\n",
    "from skimage.transform import resize\n",
    "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D,Flatten\n",
    "import requests\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60be86e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "de65cdce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global paths\n",
    "OUTPUT_DIRECTORY = \"./outputs/\"\n",
    "LABEL_DIRECTORY = \"./labels/\"\n",
    "MODEL_DIRECTORY = \"./models/\"\n",
    "MODEL_GD_ID = \"1MRbN5hXOTYnw7-71K-2vjY01uJ9GkQM5\"\n",
    "MODEL_ZIP_FILE = \"./models/models.zip\"\n",
    "IMG_DIRECTORY = \"./images/\"\n",
    "IMG_GD_ID = \"1xnK3B6K6KekDI55vwJ0vnc2IGoDga9cj\"\n",
    "IMG_ZIP_FILE = \"./images/images.zip\"\n",
    "\n",
    "# Global variables\n",
    "RAW_IMG_SIZE = (256, 256)\n",
    "IMG_SIZE = (224, 224)\n",
    "INPUT_SHAPE = (IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    "MAX_EPOCH = 10\n",
    "BATCH_SIZE = 8\n",
    "FOLDS = 5\n",
    "STOPPING_PATIENCE = 32\n",
    "LR_PATIENCE = 16\n",
    "INITIAL_LR = 0.0001\n",
    "CLASSES = [0, 1, 2, 3, 4, 5, 6, 7, 8]\n",
    "CLASS_NAMES = ['Chinee Apple',\n",
    "               'Lantana',\n",
    "               'Parkinsonia',\n",
    "               'Parthenium',\n",
    "               'Prickly Acacia',\n",
    "               'Rubber Vine',\n",
    "               'Siam Weed',\n",
    "               'Snake Weed',\n",
    "               'Negatives']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "86d1dce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def crop(img, size):\n",
    "    \"\"\"\n",
    "    Crop the image concentrically to the desired size.\n",
    "    :param img: Input image\n",
    "    :param size: Required crop image size\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    (h, w, c) = img.shape\n",
    "    x = int((w - size[0]) / 2)\n",
    "    y = int((h - size[1]) / 2)\n",
    "    return img[y:(y + size[1]), x:(x + size[0]), :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d02c42ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_generator(batches, size):\n",
    "    \"\"\"\n",
    "    Take as input a Keras ImageGen (Iterator) and generate random\n",
    "    crops from the image batches generated by the original iterator\n",
    "    :param batches: Batches of images to be cropped\n",
    "    :param size: Size to be cropped to\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        (b, h, w, c) = batch_x.shape\n",
    "        batch_crops = np.zeros((b, size[0], size[1], c))\n",
    "        for i in range(b):\n",
    "            batch_crops[i] = crop(batch_x[i], (size[0], size[1]))\n",
    "        yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4263369",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5 - 20220109-205834\n",
      "Found 10501 validated image filenames.\n",
      "Found 3501 validated image filenames.\n",
      "Found 3507 validated image filenames.\n",
      "Epoch 1/10\n",
      "  41/1312 [..............................] - ETA: 32:00 - loss: -9.5044 - categorical_accuracy: 0.0305"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8096/4268981560.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[0mlast_best_epochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mglobal_epoch\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mMAX_EPOCH\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 118\u001b[1;33m             history = model.fit(\n\u001b[0m\u001b[0;32m    119\u001b[0m                 \u001b[0mtrain_data_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain_image_count\u001b[0m \u001b[1;33m//\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1219\u001b[0m               \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtmp_logs\u001b[0m  \u001b[1;31m# No error, now safe to assign to logs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1220\u001b[0m               \u001b[0mend_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep_increment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1221\u001b[1;33m               \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mend_step\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1222\u001b[0m               \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop_training\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m                 \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m    434\u001b[0m     \"\"\"\n\u001b[0;32m    435\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_should_call_train_batch_hooks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 436\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook\u001b[1;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[0;32m    293\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_begin_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'end'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 295\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_end_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    296\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    297\u001b[0m       raise ValueError(\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_end_hook\u001b[1;34m(self, mode, batch, logs)\u001b[0m\n\u001b[0;32m    314\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_time\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 316\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_batch_hook_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhook_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    317\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_times\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_batches_for_timing_check\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_call_batch_hook_helper\u001b[1;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[0;32m    352\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m       \u001b[0mhook\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallback\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 354\u001b[1;33m       \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_timing\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36mon_train_batch_end\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1031\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_train_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1032\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_batch_update_progbar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1034\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mon_test_batch_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\callbacks.py\u001b[0m in \u001b[0;36m_batch_update_progbar\u001b[1;34m(self, batch, logs)\u001b[0m\n\u001b[0;32m   1102\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1103\u001b[0m       \u001b[1;31m# Only block async when verbose = 1.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1104\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1105\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogbar\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mseen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfinalize\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36msync_to_numpy_or_python_type\u001b[1;34m(tensors)\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 554\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    556\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\keras\\utils\\tf_utils.py\u001b[0m in \u001b[0;36m_to_single_numpy_or_python_type\u001b[1;34m(t)\u001b[0m\n\u001b[0;32m    548\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_to_single_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    549\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 550\u001b[1;33m       \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    551\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m  \u001b[1;31m# Don't turn ragged or sparse tensors to NumPy.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1147\u001b[0m     \"\"\"\n\u001b[0;32m   1148\u001b[0m     \u001b[1;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1149\u001b[1;33m     \u001b[0mmaybe_arr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1150\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1151\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python38\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1113\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1114\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1115\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1116\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1117\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#def cross_validate(model_name):\n",
    "\n",
    "    # K fold cross validation, saving outputs for each fold\n",
    "for k in range(FOLDS):\n",
    "\n",
    "        # Create new output directory for individual folds from timestamp\n",
    "        timestamp = datetime.fromtimestamp(time()).strftime('%Y%m%d-%H%M%S')\n",
    "        print('Fold {}/{} - {}'.format(k + 1, FOLDS, timestamp))\n",
    "        output_directory = \"{}{}/\".format(OUTPUT_DIRECTORY, timestamp)\n",
    "        if not os.path.exists(output_directory):\n",
    "            os.makedirs(output_directory)\n",
    "\n",
    "        # Prepare training, validation and testing labels for kth fold\n",
    "        train_label_file = \"{}train_subset{}.csv\".format(LABEL_DIRECTORY, k)\n",
    "        val_label_file = \"{}val_subset{}.csv\".format(LABEL_DIRECTORY, k)\n",
    "        test_label_file = \"{}test_subset{}.csv\".format(LABEL_DIRECTORY, k)\n",
    "        train_dataframe = pd.read_csv(train_label_file)\n",
    "        val_dataframe = pd.read_csv(val_label_file)\n",
    "        test_dataframe = pd.read_csv(test_label_file)\n",
    "        train_image_count = train_dataframe.shape[0]\n",
    "        val_image_count = train_dataframe.shape[0]\n",
    "        test_image_count = test_dataframe.shape[0]\n",
    "\n",
    "        # Training image augmentation\n",
    "        train_data_generator = ImageDataGenerator(\n",
    "            rescale=1. / 255,\n",
    "            fill_mode=\"constant\",\n",
    "            shear_range=0.2,\n",
    "            zoom_range=(0.5, 1),\n",
    "            horizontal_flip=True,\n",
    "            rotation_range=360,\n",
    "            channel_shift_range=25,\n",
    "            brightness_range=(0.75, 1.25))\n",
    "\n",
    "        # Validation image augmentation\n",
    "        val_data_generator = ImageDataGenerator(\n",
    "            rescale=1. / 255,\n",
    "            fill_mode=\"constant\",\n",
    "            shear_range=0.2,\n",
    "            zoom_range=(0.5, 1),\n",
    "            horizontal_flip=True,\n",
    "            rotation_range=360,\n",
    "            channel_shift_range=25,\n",
    "            brightness_range=(0.75, 1.25))\n",
    "\n",
    "        # No testing image augmentation (except for converting pixel values to floats)\n",
    "        test_data_generator = ImageDataGenerator(rescale=1. / 255)\n",
    "\n",
    "        # Load train images in batches from directory and apply augmentations\n",
    "        train_data_generator = train_data_generator.flow_from_dataframe(\n",
    "            train_dataframe,\n",
    "            IMG_DIRECTORY,\n",
    "            x_col='Filename',\n",
    "            y_col=\"Label\",\n",
    "            target_size=RAW_IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            #has_ext=True,\n",
    "            classes=[0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
    "            class_mode='raw')\n",
    "\n",
    "        # Load validation images in batches from directory and apply rescaling\n",
    "        val_data_generator = val_data_generator.flow_from_dataframe(\n",
    "            val_dataframe,\n",
    "            IMG_DIRECTORY,\n",
    "            x_col=\"Filename\",\n",
    "            y_col=\"Label\",\n",
    "            target_size=RAW_IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            #has_ext=True,\n",
    "            classes=[0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
    "            class_mode='raw')\n",
    "\n",
    "        # Load test images in batches from directory and apply rescaling\n",
    "        test_data_generator = test_data_generator.flow_from_dataframe(\n",
    "            test_dataframe,\n",
    "            IMG_DIRECTORY,\n",
    "            x_col=\"Filename\",\n",
    "            y_col=\"Label\",\n",
    "            target_size=IMG_SIZE,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            #has_ext=True,\n",
    "            shuffle=False,\n",
    "            classes=[0, 1, 2, 3, 4, 5, 6, 7, 8],\n",
    "            class_mode='raw')\n",
    "\n",
    "        # Crop augmented images from 256x256 to 224x224\n",
    "        train_data_generator = crop_generator(train_data_generator, IMG_SIZE)\n",
    "        val_data_generator = crop_generator(val_data_generator, IMG_SIZE)\n",
    "\n",
    "        # Load ImageNet pre-trained model with no top, either InceptionV3 or ResNet50\n",
    "        #if model_name == \"resnet\":\n",
    "           # base_model = ResNet50(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
    "        #elif model_name == \"inception\":\n",
    "        base_model = InceptionV3(weights='imagenet', include_top=False, input_shape=INPUT_SHAPE)\n",
    "        x = base_model.output\n",
    "        # Add a global average pooling layer\n",
    "        x = GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        # Add fully connected output layer with sigmoid activation for multi label classification\n",
    "        outputs = Dense(len(CLASSES), activation='sigmoid', name='fc9')(x)\n",
    "        # Assemble the modified model\n",
    "        f=Flatten()(outputs)\n",
    "        model = Model(inputs=base_model.input, outputs=f)\n",
    "\n",
    "        # Checkpoints for training\n",
    "        model_checkpoint = ModelCheckpoint(output_directory + \"lastbest-0.hdf5\", verbose=1, save_best_only=True)\n",
    "        early_stopping = EarlyStopping(patience=STOPPING_PATIENCE, restore_best_weights=True)\n",
    "        tensorboard = TensorBoard(log_dir=output_directory, histogram_freq=0, write_graph=True, write_images=False)\n",
    "        reduce_lr = ReduceLROnPlateau('val_loss', factor=0.5, patience=LR_PATIENCE, min_lr=0.000003125)\n",
    "        model.compile(loss='binary_crossentropy', optimizer=Adam(lr=INITIAL_LR), metrics=['categorical_accuracy'])\n",
    "        csv_logger = CSVLogger(output_directory + \"training_metrics.csv\")\n",
    "\n",
    "        # Train model until MAX_EPOCH, restarting after each early stop when learning has plateaued\n",
    "        global_epoch = 0\n",
    "        restarts = 0\n",
    "        last_best_losses = []\n",
    "        last_best_epochs = []\n",
    "        while global_epoch < MAX_EPOCH:\n",
    "            history = model.fit(\n",
    "                train_data_generator,\n",
    "                steps_per_epoch=train_image_count // BATCH_SIZE,\n",
    "                epochs=MAX_EPOCH - global_epoch,\n",
    "                validation_data=val_data_generator,\n",
    "                validation_steps=val_image_count // BATCH_SIZE,\n",
    "                callbacks=[tensorboard, model_checkpoint, early_stopping, reduce_lr, csv_logger],\n",
    "                shuffle=False)\n",
    "            last_best_losses.append(min(history.history['val_loss']))\n",
    "            last_best_local_epoch = history.history['val_loss'].index(min(history.history['val_loss']))\n",
    "            last_best_epochs.append(global_epoch + last_best_local_epoch)\n",
    "            if early_stopping.stopped_epoch == 0:\n",
    "                print(\"Completed training after {} epochs.\".format(MAX_EPOCH))\n",
    "                break\n",
    "            else:\n",
    "                global_epoch = global_epoch + early_stopping.stopped_epoch - STOPPING_PATIENCE + 1\n",
    "                print(\"Early stopping triggered after local epoch {} (global epoch {}).\".format(\n",
    "                    early_stopping.stopped_epoch, global_epoch))\n",
    "                print(\"Restarting from last best val_loss at local epoch {} (global epoch {}).\".format(\n",
    "                    early_stopping.stopped_epoch - STOPPING_PATIENCE, global_epoch - STOPPING_PATIENCE))\n",
    "                restarts = restarts + 1\n",
    "                model.compile(loss='binary_crossentropy', optimizer=Adam(lr=INITIAL_LR / 2 ** restarts),\n",
    "                              metrics=['categorical_accuracy'])\n",
    "                model_checkpoint = ModelCheckpoint(output_directory + \"lastbest-{}.hdf5\".format(restarts),\n",
    "                                                   monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "        # Save last best model info\n",
    "        with open(output_directory + \"last_best_models.csv\", 'w', newline='') as file:\n",
    "            writer = csv.writer(file, delimiter=',')\n",
    "            writer.writerow(['Model file', 'Global epoch', 'Validation loss'])\n",
    "            for i in range(restarts + 1):\n",
    "                writer.writerow([\"lastbest-{}.hdf5\".format(i), last_best_epochs[i], last_best_losses[i]])\n",
    "\n",
    "        # Load the last best model\n",
    "        model = load_model(\n",
    "            output_directory + \"lastbest-{}.hdf5\".format(last_best_losses.index(min(last_best_losses))))\n",
    "\n",
    "        # Evaluate model on test subset for kth fold\n",
    "        predictions = model.predict_generator(test_data_generator, test_image_count // BATCH_SIZE + 1)\n",
    "        y_true = test_data_generator.labels\n",
    "        y_pred = np.argmax(predictions, axis=1)\n",
    "        y_pred[np.max(predictions, axis=1) < 1 / 9] = 8  # Assign predictions worse than random guess to negative class\n",
    "\n",
    "        # Generate and print classification metrics and confusion matrix\n",
    "        print(classification_report(y_true, y_pred, labels=CLASSES, target_names=CLASS_NAMES))\n",
    "        report = classification_report(y_true, y_pred, labels=CLASSES, target_names=CLASS_NAMES, output_dict=True)\n",
    "        with open(output_directory + 'classification_report.csv', 'w') as f:\n",
    "            for key in report.keys():\n",
    "                f.write(\"%s,%s\\n\" % (key, report[key]))\n",
    "        conf_arr = confusion_matrix(y_true, y_pred, labels=CLASSES)\n",
    "        print(conf_arr)\n",
    "        np.savetxt(output_directory + \"confusion_matrix.csv\", conf_arr, delimiter=\",\")\n",
    "\n",
    "        # Clear model from GPU after each iteration\n",
    "        print(\"Finished testing fold {}\\n\".format(k + 1))\n",
    "        K.clear_session()\n",
    "        k = k + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a56ea8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825f1245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11210178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def inference(model):\n",
    "\n",
    "    # Create new output directory for saving inference times\n",
    "timestamp = datetime.fromtimestamp(time()).strftime('%Y%m%d-%H%M%S')\n",
    "output_directory = \"{}{}/\".format(OUTPUT_DIRECTORY, timestamp)\n",
    "if not os.path.exists(output_directory):\n",
    "       os.makedirs(output_directory)\n",
    "\n",
    "    # Load DeepWeeds dataframe\n",
    "dataframe = pd.read_csv(LABEL_DIRECTORY + \"labels.csv\")\n",
    "image_count = dataframe.shape[0]\n",
    "filenames = dataframe.Filename\n",
    "\n",
    "preprocessing_times = []\n",
    "inference_times = []\n",
    "for i in range(image_count):\n",
    "        # Load image\n",
    "        start_time = time()\n",
    "        img = imread(IMG_DIRECTORY + filenames[i])\n",
    "        # Resize to 224x224\n",
    "        img = resize(img, (224, 224))\n",
    "        # Map to batch\n",
    "        img = np.expand_dims(img, axis=0)\n",
    "        # Scale from int to float\n",
    "        img = img * 1./255\n",
    "        preprocessing_time = time() - start_time\n",
    "        start_time = time()\n",
    "        # Predict label\n",
    "        prediction = model.predict(img, batch_size=1, verbose=0)\n",
    "        y_pred = np.argmax(prediction, axis=1)\n",
    "        y_pred[np.max(prediction, axis=1) < 1/9] = 8\n",
    "        inference_time = time() - start_time\n",
    "        # Append times to lists\n",
    "        preprocessing_times.append(preprocessing_time)\n",
    "        inference_times.append(inference_time)\n",
    "\n",
    "    # Save inference times to csv\n",
    "with open(output_directory + \"tf_inference_times.csv\", 'w', newline='') as file:\n",
    "        writer = csv.writer(file, delimiter=',')\n",
    "        writer.writerow(['Filename', 'Preprocessing time (ms)', 'Inference time (ms)'])\n",
    "        for i in range(image_count):\n",
    "            writer.writerow([filenames[i], preprocessing_times[i] * 1000, inference_times[i] * 1000])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe68567f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
